{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pd\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.image import rgb_to_grayscale\n",
    "\n",
    "\n",
    "# Reshaping \n",
    "from tensorflow import reshape\n",
    "from tensorflow.image import resize_with_pad\n",
    "\n",
    "\n",
    "# Modelling \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Data From the Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "PATH = os.getcwd() + '/../../src/data/chest_xray/'\n",
    "\n",
    "\n",
    "# importing training normal data\n",
    "norm_train_path = PATH+'train/NORMAL/'\n",
    "norm_train_batch = os.listdir(norm_train_path) \n",
    "\n",
    "norm_train = []\n",
    "norm_errors = []\n",
    "for image_name in (norm_train_batch[:10]): \n",
    "    img_path = norm_train_path + image_name \n",
    "    try:\n",
    "        x = image.load_img(img_path) \n",
    "        # preprocessing if required \n",
    "        norm_train.append(x) \n",
    "    except:\n",
    "        norm_errors.append(image_name)\n",
    "    \n",
    "    \n",
    "# importing training pnuemonia data\n",
    "pnue_train_path = PATH + 'train/PNEUMONIA/'\n",
    "pnue_train_batch = os.listdir(pnue_train_path)\n",
    "\n",
    "pnue_train = []\n",
    "pnue_errors = []\n",
    "for image_name in pnue_train_batch[:10]:\n",
    "    img_path = pnue_train_path + image_name\n",
    "    try:\n",
    "        x = image.load_img(img_path)\n",
    "        pnue_train.append(x)\n",
    "    except:\n",
    "        pnue_errors.append(image_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_image(PATH, image_name):\n",
    "    \"\"\"\n",
    "    PATH --> str: Relative path to image directoy\n",
    "    image_name --> str: Name of the image to load\n",
    "    \n",
    "    Returns:\n",
    "    PIL image\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create path to file\n",
    "    img_path = PATH + \"/\" + image_name\n",
    "    \n",
    "    # load file and return pil\n",
    "    return image.load_img(img_path) \n",
    "\n",
    "def grayscale_and_resize(PIL, shape=(256,256), padding=False):\n",
    "    \"\"\"\n",
    "    This is the preprocessing function that will take the raw jpeg, gray scale it, resize it and \n",
    "    turn it into an array\n",
    "    \n",
    "    \n",
    "    PIL --> PIL object\n",
    "    shape --> tuple: size of the final array\n",
    "    padding --> bool: if True, will use tf.resize_with_pad\n",
    "    \"\"\"\n",
    "    if padding:\n",
    "        gray_image = rgb_to_grayscale(PIL)\n",
    "        resized_image_arr = resize_with_pad(gray_image, target_height=shape[0], target_width=shape[1])\n",
    "    else:\n",
    "        resized_image_arr = img_to_array(PIL.convert(mode = 'L').resize(shape))\n",
    "    \n",
    "    return resized_image_arr\n",
    "\n",
    "\n",
    "def import_image_to_array(RELPATH,\n",
    "         dir_names = ['train', 'test', 'val'],\n",
    "         sub_dir_names = ['NORMAL', 'PNEUMONIA'],\n",
    "         padding=False,\n",
    "         shape=(256,256), test=False):\n",
    "    \"\"\"\n",
    "    This function loads all train, test and validation data into a dictionary of images\n",
    "    =====================================================================================\n",
    "    RELPATH --> str: The relative path to the cwd to the directory containing image directories\n",
    "    eg '../../src/data/chest_xray'\n",
    "    =====================================================================================\n",
    "    dir_names --> list, str: The names of the subdirectories containing the images\n",
    "    eg ['train', 'test', 'val'] <-- default\n",
    "    =====================================================================================\n",
    "    sub_dir_names --> list -> str: names of the subdirectory containg postivie and negative cases\n",
    "    \n",
    "    =====================================================================================\n",
    "    padding  --> bool: Whether you want the reshaping to be padded to or not\n",
    "    \n",
    "    =====================================================================================\n",
    "    shape --> tuple-> int: The final shape of the tensor array\n",
    "    \n",
    "    returns\n",
    "    \n",
    "    dict --> str:list -> tuple -> (tf.array, bool)\n",
    "    A dictionary where the keys are the dir_names and the values are lists containing tuple where \n",
    "    the first index is the tf.array and the second is a boolian, True if class is pnuemonia, false otherwise.\n",
    "    \"\"\"\n",
    "    # test relative path works!! \n",
    "    PATH = os.getcwd() + RELPATH\n",
    "    \n",
    "    try:\n",
    "        os.listdir(PATH)\n",
    "        print(\"You're relative directory is good, proceeding to import files...\", end=\"\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(f\"Your relative path directory is not pointing to the correct location. Double check your input \\n\")\n",
    "        print(\"Terminating Program\", end='\\n')\n",
    "        print(\"=======================================================================================\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # instantiate a dict object and populate the keys\n",
    "    image_dict = {}\n",
    "    for name in dir_names:\n",
    "        image_dict[name] = []\n",
    "        \n",
    "        print(f\"Loading images from {name}\", end='\\n')\n",
    "        \n",
    "        \n",
    "        # For each subdirectory, get all of the images and append to dictionary\n",
    "        for sub_dir in sub_dir_names:\n",
    "            subPATH = PATH + name + \"/\" + sub_dir\n",
    "            # list of all image names in the subdirectory\n",
    "            image_batch = os.listdir(subPATH)\n",
    "            \n",
    "            for image in image_batch:\n",
    "                # import the image in pil format\n",
    "                pil = import_image(subPATH, image)\n",
    "                # gray scale and reshape the image turning it into an array\n",
    "                gray_resized_pil = grayscale_and_resize(pil, shape=shape, padding=padding)\n",
    "                \n",
    "                # center the pixels\n",
    "                centered_array = gray_resized_pil/255\n",
    "                \n",
    "                # append to the image_dict\n",
    "                flag = 1\n",
    "                if sub_dir == 'NORMAL':\n",
    "                    flag = 0\n",
    "                \n",
    "                image_dict[name].append((centered_array, flag))\n",
    "                \n",
    "            \n",
    "                # if this is just a test case, break out of this loop so we get one from each class\n",
    "                if test == True:\n",
    "                    break\n",
    "            \n",
    "            print(f\"Finished loading images from {sub_dir}\", end=\"\\n\")\n",
    "\n",
    "        print()\n",
    "    \n",
    "    return image_dict               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're relative directory is good, proceeding to import files...\n",
      "\n",
      "Loading images from train\n",
      "Finished loading images from NORMAL\n",
      "Finished loading images from PNEUMONIA\n",
      "\n",
      "Loading images from test\n",
      "Finished loading images from NORMAL\n",
      "Finished loading images from PNEUMONIA\n",
      "\n",
      "Loading images from val\n",
      "Finished loading images from NORMAL\n",
      "Finished loading images from PNEUMONIA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = import_image_to_array('/../../src/data/chest_xray/',test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for class imbalance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate inverse \n",
    "pnue_frequency = sum(y_train)/len(y_train)\n",
    "inv_pnue_frequency = 1/pnue_frequency\n",
    "\n",
    "normal_frequency = (len(y_train)-sum(y_train))/len(y_train)\n",
    "inv_normal_frequency = 1/normal_frequency\n",
    "\n",
    "weights = {\n",
    "    0: inv_normal_frequency,\n",
    "    1: inv_pnue_frequency\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([i[1] for i in images['train']])\n",
    "X_train = np.array([i[0] for i in images['train']])  \n",
    "X_test = np.array([i[0] for i in images['test']])\n",
    "y_test = np.array([i[1] for i in images['test']])\n",
    "X_val = np.array([i[0] for i in images['val']])\n",
    "y_val = np.array([i[1] for i in images['val']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.Sequential()\n",
    "adam = Adam()\n",
    "# Input layer conv\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256,  1)))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# First hidden layer conv\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Flatten())\n",
    "\n",
    "# Added first dense layer\n",
    "cnn.add(layers.Dense(32, activation='relu'))\n",
    "cnn.add(layers.Dropout(.2))\n",
    "\n",
    "# Add Second Layer\n",
    "cnn.add(layers.Dense(16, activation='relu'))\n",
    "cnn.add(layers.Dropout(.1))\n",
    "\n",
    "cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "105/105 [==============================] - 486s 5s/step - loss: 0.8032 - acc: 0.8269 - val_loss: 0.5055 - val_acc: 0.7981\n",
      "Epoch 2/5\n",
      "105/105 [==============================] - 445s 4s/step - loss: 0.3573 - acc: 0.9319 - val_loss: 1.0566 - val_acc: 0.7740\n",
      "Epoch 3/5\n",
      "105/105 [==============================] - 492s 5s/step - loss: 0.2740 - acc: 0.9471 - val_loss: 1.0106 - val_acc: 0.7933\n",
      "Epoch 4/5\n",
      "105/105 [==============================] - 460s 4s/step - loss: 0.2289 - acc: 0.9492 - val_loss: 0.6708 - val_acc: 0.7837\n",
      "Epoch 5/5\n",
      "105/105 [==============================] - 452s 4s/step - loss: 0.2158 - acc: 0.9563 - val_loss: 0.9421 - val_acc: 0.7532\n"
     ]
    }
   ],
   "source": [
    "cnn1 = cnn.fit(X_train, y_train,\n",
    "               epochs=5,\n",
    "               batch_size=50,\n",
    "               validation_data = (X_test, y_test), \n",
    "               class_weight=weights,\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model. The validation accuracy with the test data is low, but let's look at the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1021 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "validation = cnn.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.52587890625e-05"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/2020-12-011.HDF5/assets\n"
     ]
    }
   ],
   "source": [
    "today = str(datetime.today()).split()[0]\n",
    "directory = \"../../models/\"\n",
    "model_id = \"tim-1\"\n",
    "file = directory+today+model_number+\".HDF5\"\n",
    "cnn.save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
